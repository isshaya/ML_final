{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMVgsr1x3Yjn"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "fbebffUO1OvP",
        "outputId": "e4cc6efb-49eb-4f89-8c22-4e683605841f"
      },
      "outputs": [],
      "source": [
        "%pip install pandas numpy\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('final_task7_dataset.csv')\n",
        "\n",
        "# Clean the dataset\n",
        "df = df.dropna(subset=['review_text'])\n",
        "df = df[df['review_text'].str.strip() != '']\n",
        "df = df.dropna(subset=['rating_x'])\n",
        "\n",
        "# Reset the index\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {df.shape}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMpKn74L2Rdz",
        "outputId": "1429453f-f6f8-444e-ff45-e99be7818ee6"
      },
      "outputs": [],
      "source": [
        "%pip install nltk\n",
        "%pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "IT1pCM5i2Ssf",
        "outputId": "76025624-450e-49bf-d95d-d8a57c28aa7a"
      },
      "outputs": [],
      "source": [
        "# NLP libraries\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "#preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Tokenize and remove stopwords\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_stop and not token.is_punct]\n",
        "    # Rejoin tokens into clean string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "df['processed_text'] = df['review_text'].apply(preprocess_text)\n",
        "\n",
        "print(f\"Shape after processing: {df.shape}\")\n",
        "df[['review_text', 'processed_text']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqnZX-qs3S3C"
      },
      "source": [
        "## TF-IDF vectorization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfXhurKl3936",
        "outputId": "3d0b2863-7f26-4438-dc68-068028f0d480"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-learn pandas\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# TF-IDF Vectorization on reviews grouped by place\n",
        "grouped = df.groupby('place_name')['processed_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "tfidf_matrix = tfidf.fit_transform(grouped['processed_text'])\n",
        "\n",
        "print(f\"TF-IDF feature matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "# Cosine Similarity\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(f\"Cosine similarity matrix shape: {cosine_sim.shape}\")\n",
        "\n",
        "# Reverse index\n",
        "indices = pd.Series(grouped.index, index=grouped['place_name']).drop_duplicates()\n",
        "\n",
        "# Reccomender function\n",
        "def recommend_places(place_name, cosine_sim=cosine_sim, df=grouped):\n",
        "    if place_name not in indices:\n",
        "        return f\"Place '{place_name}' not found in the dataset.\"\n",
        "     \n",
        "    idx = indices[place_name]\n",
        "\n",
        "    # Pairwise similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    sim_scores = sim_scores[1:min(6, len(sim_scores))]  # added min(6, len(sim_scores)) to not go past available data\n",
        "\n",
        "    place_indices = [i[0] for i in sim_scores]\n",
        "    return df['place_name'].iloc[place_indices]\n",
        "\n",
        "# Test Recommender\n",
        "recommended_bars = recommend_places('The Dead Rabbit')\n",
        "print(\"Top recommended similar bars:\")\n",
        "print(recommended_bars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIXLO0i6biyH"
      },
      "outputs": [],
      "source": [
        "# Save cosine similarity matrix\n",
        "np.save(\"cosine_sim.npy\", cosine_sim)\n",
        "\n",
        "# Save the reverse index mapping\n",
        "indices.to_pickle(\"place_indices.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDoKx51l4bAv"
      },
      "source": [
        "## Word2Vec vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzkyOKmq4b04",
        "outputId": "b3ee356e-8ce8-4aae-cbb3-40b7d87dc2a5"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenizing processed text\n",
        "df['tokens'] = df['processed_text'].apply(lambda x: x.split())\n",
        "\n",
        "# Training Word2Vec model\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=df['tokens'],\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=2,     \n",
        "    workers=4,      \n",
        "    sg=1            \n",
        ")\n",
        "\n",
        "# Function to average word vectors for a document\n",
        "def document_vector(doc):\n",
        "    doc = [word for word in doc if word in w2v_model.wv]\n",
        "    if len(doc) == 0:\n",
        "        return np.zeros(100)  # Return zero vector if no valid words\n",
        "    return np.mean(w2v_model.wv[doc], axis=0)\n",
        "\n",
        "# Creating document vectors\n",
        "doc_vectors = np.array([document_vector(tokens) for tokens in df['tokens']])\n",
        "\n",
        "print(f\"Shape of document vectors: {doc_vectors.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3TRvP-X_0wa",
        "outputId": "cc988dba-fd97-49b6-dab6-d0f724ae4bd5"
      },
      "outputs": [],
      "source": [
        "# Shape: (2599, 100), where each row is a document vector\n",
        "\n",
        "# Convert to DataFrame\n",
        "word2vec_df = pd.DataFrame(doc_vectors)\n",
        "word2vec_df.columns = [f\"w2v_{i}\" for i in range(word2vec_df.shape[1])]\n",
        "\n",
        "\n",
        "df_reset = df.reset_index(drop=True)\n",
        "# assert len(df_reset) == len(word2vec_df), \"Mismatch in row count!\" # used to check alignment\n",
        "\n",
        "\n",
        "df_word2vec = pd.concat([df_reset, word2vec_df], axis=1)\n",
        "\n",
        "\n",
        "df_word2vec.to_csv(\"word2vec_reviews.csv\", index=False)\n",
        "\n",
        "print(\"Word2Vec document vectors saved to 'word2vec_reviews.csv'\")\n",
        "print(f\"Final shape: {df_word2vec.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHS6J1oOEwWe"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85NNtUqREyu8",
        "outputId": "d93f784e-dcd8-4692-d073-84df57a8b4b4"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "df['tokens'] = df['processed_text'].apply(lambda x: x.split())\n",
        "\n",
        "# Creating dictionary and corpus for LDA\n",
        "dictionary = corpora.Dictionary(df['tokens'])\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)  \n",
        "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
        "\n",
        "NUM_TOPICS = 10\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=NUM_TOPICS,\n",
        "    passes=10,\n",
        "    random_state=42,\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "# Top words for each topic without probabilities \n",
        "NUM_TOP_WORDS = 10\n",
        "for i in range(lda_model.num_topics):\n",
        "    words = lda_model.show_topic(i, topn=NUM_TOP_WORDS)\n",
        "    print(f\"Topic #{i}: \" + \", \".join([word for word, prob in words]))\n",
        "\n",
        "\n",
        "# Top words with probabilities\n",
        "for i, topic in lda_model.show_topics(num_topics=NUM_TOPICS, formatted=True):\n",
        "    print(f\"Topic #{i}:\")\n",
        "    print(topic)\n",
        "    print()\n",
        "\n",
        "# Assigning dominant topic to each review\n",
        "def get_dominant_topic(lda_model, bow):\n",
        "    topics = lda_model.get_document_topics(bow)\n",
        "    if topics:\n",
        "        return sorted(topics, key=lambda x: x[1], reverse=True)[0][0]\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "df['lda_dominant_topic'] = [get_dominant_topic(lda_model, doc) for doc in corpus]\n",
        "\n",
        "NUM_TOP_WORDS = 5\n",
        "topic_keywords_map = {\n",
        "    topic_id: \", \".join([word for word, _ in lda_model.show_topic(topic_id, topn=NUM_TOP_WORDS)])\n",
        "    for topic_id in range(lda_model.num_topics)\n",
        "}\n",
        "\n",
        "# Mapping keywords to reviews\n",
        "df['topic_keywords'] = df['lda_dominant_topic'].map(topic_keywords_map)\n",
        "\n",
        "df.to_csv(\"lda_labeled_reviews.csv\", index=False)\n",
        "print(\"LDA topics assigned and saved to 'lda_labeled_reviews.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot of Dominant Topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uMSPM0EuFVhm",
        "outputId": "a7549f3b-60cc-465f-cbed-9b2cfd3fdd55"
      },
      "outputs": [],
      "source": [
        "%pip install matplotlib seaborn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "topic_counts = df['lda_dominant_topic'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=topic_counts.index, y=topic_counts.values, palette='muted')\n",
        "plt.xlabel('LDA Topic #')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.title('Distribution of Dominant LDA Topics')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx7u6l_oAUg9"
      },
      "source": [
        "## Reccomendation System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkGvlQqnCLbV",
        "outputId": "cbef101e-5913-455f-bcc4-893d93f0ac17"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-surprise\n",
        "\n",
        "df_filtered = df[['reviewer_id', 'place_id', 'rating_x']].dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Content-Based TF-IDF Recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xsuvHmkDmNB",
        "outputId": "58e34095-af9f-4b2b-bd27-350c32952155"
      },
      "outputs": [],
      "source": [
        "grouped_reviews = df.groupby('place_name')['processed_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# TF IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "tfidf_matrix = tfidf.fit_transform(grouped_reviews['processed_text'])\n",
        "\n",
        "# Cosine similarity\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "place_indices = pd.Series(grouped_reviews.index, index=grouped_reviews['place_name']).drop_duplicates()\n",
        "\n",
        "# Recommender\n",
        "def recommend_places(place_name, cosine_sim=cosine_sim, df=grouped_reviews):\n",
        "    idx = place_indices[place_name]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:6]\n",
        "    place_indices_top = [i[0] for i in sim_scores]\n",
        "    return df['place_name'].iloc[place_indices_top]\n",
        "\n",
        "# Examples\n",
        "print(\"Content-Based Recommendations:\")\n",
        "print(recommend_places('The Dead Rabbit'))\n",
        "print(recommend_places('Old Town Bar'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collaborative Filtering with Surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfqmNIPUDo-7",
        "outputId": "e68a7d20-7374-4acb-e8be-aba07455cc8e"
      },
      "outputs": [],
      "source": [
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "# Load the dataset into Surprise\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(df_filtered[['reviewer_id', 'place_id', 'rating_x']], reader)\n",
        "\n",
        "# Split into training and testing sets\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model\n",
        "model = SVD()\n",
        "model.fit(trainset)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.test(testset)\n",
        "print(\"\\nCollaborative Filtering Evaluation:\")\n",
        "print(\"RMSE:\", accuracy.rmse(predictions))\n",
        "print(\"MAE :\", accuracy.mae(predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Top-N Recommender for User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPHZm9xXDpol",
        "outputId": "9cf99082-c41d-418e-c86d-be5c82033fdd"
      },
      "outputs": [],
      "source": [
        "def get_top_n_recommendations(user_id, model, df_all, n=5):\n",
        "    all_bars = df_all['place_id'].unique()\n",
        "    rated_bars = df_all[df_all['reviewer_id'] == user_id]['place_id'].unique()\n",
        "    bars_to_predict = [bar for bar in all_bars if bar not in rated_bars]\n",
        "    predictions = [model.predict(user_id, bar) for bar in bars_to_predict]\n",
        "    top_n_preds = sorted(predictions, key=lambda x: x.est, reverse=True)[:n]\n",
        "    place_id_to_name = dict(zip(df_all['place_id'], df_all['place_name']))\n",
        "    results = [(place_id_to_name.get(pred.iid, pred.iid), round(pred.est, 2)) for pred in top_n_preds]\n",
        "    return results\n",
        "\n",
        "# Example\n",
        "sample_user = df_filtered['reviewer_id'].iloc[0]\n",
        "print(f\"\\nTop-N Recommendations for User: {sample_user}\")\n",
        "for place, score in get_top_n_recommendations(sample_user, model, df):\n",
        "    print(f\"{place} — Predicted rating: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Top-N Recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoVKKd6LGh2L",
        "outputId": "e6a81218-d650-4679-afad-8e291ef6d0b3"
      },
      "outputs": [],
      "source": [
        "# Real user + bar IDs\n",
        "user_id = '115364016342485480165'\n",
        "bar_id = 'ChIJXz1QXE5ZwokRLwJIVmQhyEc'\n",
        "\n",
        "# Lookup dictionary for place_id → place_name\n",
        "place_id_to_name = dict(zip(df['place_id'], df['place_name']))\n",
        "\n",
        "# Predicting single rating\n",
        "pred = model.predict(user_id, bar_id)\n",
        "bar_name = place_id_to_name.get(bar_id, \"Unknown Bar\")\n",
        "print(f\"Predicted rating for user '{user_id}' on bar '{bar_name}': {round(pred.est, 2)}\")\n",
        "\n",
        "# Get and print Top-N recommendations for the same user\n",
        "top_recs = get_top_n_recommendations(user_id, model, df, n=5)\n",
        "print(f\"\\nTop 5 recommended bars for user '{user_id}':\")\n",
        "for bar_name, score in top_recs:\n",
        "    print(f\"{bar_name} — Predicted rating: {score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
